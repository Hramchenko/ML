{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW_mod_plt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fuzWrhly9MxT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "asKj5aqL-7RE",
        "colab_type": "code",
        "outputId": "b84bcbec-408e-4abf-cfaf-078097687f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "V0DqE2vM97cP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "fashion_mnist_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.2860,), (0.3530,)),\n",
        "           ])\n",
        "\n",
        "def make_mnist(l, batch_size=50, valid=0, shuffle=True, transform=fashion_mnist_transform, path=\"./\"):\n",
        "    test_data = l(path, train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    train_data = l(path, train=True, download=True, transform=transform)\n",
        "    if valid > 0:\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = num_train-valid\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "        return train_loader, valid_loader, test_loader\n",
        "    else:\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "def fashion_mnist(batch_size=50, valid=0, shuffle=True, transform=fashion_mnist_transform, path='./FashionMNIST_data'):\n",
        "    return make_mnist(datasets.FashionMNIST, batch_size, valid, shuffle, transform, path)\n",
        "\n",
        "def plot_mnist(images, shape):\n",
        "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
        "    for j in range(1, len(images) + 1):\n",
        "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
        "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    plt.show()\n",
        "    \n",
        "def plot_graphs(log, tpe='loss'):\n",
        "    keys = log.keys()\n",
        "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
        "    epochs = {k:range(len(log[k])) for k in keys}\n",
        "    \n",
        "    if tpe == 'loss':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
        "        plt.title('errors')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('error')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()\n",
        "    elif tpe == 'accuracy':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
        "        plt.title('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZ5G0vvH8eHN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJfNnaX18eHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = fashion_mnist(valid=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6q3zVh7H8eHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, batchnorm=False, dropout=False, xa=False, lr=1e-4, l2=0.):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128*3)\n",
        "        self.fc2 = nn.Linear(128*3, 200)\n",
        "        self.fc3 = nn.Linear(200, 10)\n",
        "        if xa:\n",
        "            nn.init.xavier_uniform_(self.fc1.weight)\n",
        "            nn.init.xavier_uniform_(self.fc2.weight)\n",
        "            nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        if batchnorm:\n",
        "            self.bn = nn.BatchNorm1d(128*3)\n",
        "        self.batchnorm = batchnorm\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        if self.batchnorm:\n",
        "            x = self.bn(x)\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        if self.dropout:\n",
        "            x = F.dropout(x, 0.5)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HaoG05Du8eHY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, models):\n",
        "    train_size = len(train_loader.sampler)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        for k, model in models.items():\n",
        "            model.optim.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, target)\n",
        "            loss.backward()\n",
        "            model.optim.step()\n",
        "            \n",
        "        if batch_idx % 200 == 0:\n",
        "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
        "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "            print(line + losses)\n",
        "            \n",
        "    else:\n",
        "        batch_idx += 1\n",
        "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
        "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "        print(line + losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p0N0Ob2bBKIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6IAmSIo8eHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "models = {'xa_bn_do': Net(True, True, xa=True, l2=3e-5)}\n",
        "#models = {'xa': Net(False, False, xa=True, l2=1e-5)}\n",
        "#models = {'xa': Net(False, False, xa=True, l2=1e-5), 'xa_bn': Net(True, False, xa=True, l2=1e-5)}\n",
        "#models = {'default': Net(False, False), 'xa': Net(False, False, xa=True)}\n",
        "#models = {'default': Net(False, False), 'bn': Net(True, False), 'drop': Net(False, True), 'both': Net(True, True)}\n",
        "train_log = {k: [] for k in models}\n",
        "test_log = {k: [] for k in models}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtaUHWB98eHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(models, loader, header, log=None):\n",
        "    test_size = len(loader.sampler)\n",
        "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
        "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
        "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
        "\n",
        "    test_loss = {k: 0. for k in models}\n",
        "    correct = {k: 0. for k in models}\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            # output = {k: m(data) for m in models}\n",
        "            for k, m in models.items():\n",
        "                output = m(data)\n",
        "                test_loss[k] += m.loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    \n",
        "    for k in models:\n",
        "        test_loss[k] /= test_size\n",
        "    correct_pct = {k: 100. * correct[k] / test_size for k in correct}\n",
        "    lines = '\\n'.join([line(k, test_loss[k], correct[k], correct_pct[k]) for k in models]) + '\\n'\n",
        "    report = header + ' set:\\n' + lines\n",
        "    if log is not None:\n",
        "        for k in models:\n",
        "            log[k].append((test_loss[k], correct_pct[k]))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "6jmGcngA8eHh",
        "colab_type": "code",
        "outputId": "b7446484-245c-4de1-80bd-f3257cfcaa46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7144
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 101):\n",
        "    for model in models.values():\n",
        "        model.train()\n",
        "    train(epoch, models)\n",
        "    for model in models.values():\n",
        "        model.eval()\n",
        "    test(models, valid_loader, 'Test', test_log)\n",
        "    test(models, train_loader, 'Train', train_log)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLosses xa_bn_do: 2.477974\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tLosses xa_bn_do: 0.711717\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLosses xa_bn_do: 0.611760\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tLosses xa_bn_do: 0.450157\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLosses xa_bn_do: 0.340327\n",
            "Train Epoch: 1 [50000/50000 (100%)]\tLosses xa_bn_do: 0.364652\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3979\tAccuracy: 8597/10000 (85%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.3787\tAccuracy: 43339/50000 (86%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLosses xa_bn_do: 0.469031\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tLosses xa_bn_do: 0.258397\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLosses xa_bn_do: 0.312965\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tLosses xa_bn_do: 0.214136\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLosses xa_bn_do: 0.539847\n",
            "Train Epoch: 2 [50000/50000 (100%)]\tLosses xa_bn_do: 0.359169\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3680\tAccuracy: 8717/10000 (87%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.3326\tAccuracy: 44058/50000 (88%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLosses xa_bn_do: 0.326768\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tLosses xa_bn_do: 0.146366\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLosses xa_bn_do: 0.362299\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tLosses xa_bn_do: 0.327265\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLosses xa_bn_do: 0.429609\n",
            "Train Epoch: 3 [50000/50000 (100%)]\tLosses xa_bn_do: 0.428692\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3415\tAccuracy: 8789/10000 (87%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.2916\tAccuracy: 44792/50000 (89%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLosses xa_bn_do: 0.246885\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tLosses xa_bn_do: 0.363299\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tLosses xa_bn_do: 0.308992\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tLosses xa_bn_do: 0.133113\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tLosses xa_bn_do: 0.264634\n",
            "Train Epoch: 4 [50000/50000 (100%)]\tLosses xa_bn_do: 0.216269\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3326\tAccuracy: 8825/10000 (88%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.2715\tAccuracy: 45102/50000 (90%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLosses xa_bn_do: 0.193098\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tLosses xa_bn_do: 0.202486\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tLosses xa_bn_do: 0.281294\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tLosses xa_bn_do: 0.412538\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tLosses xa_bn_do: 0.234911\n",
            "Train Epoch: 5 [50000/50000 (100%)]\tLosses xa_bn_do: 0.305317\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3292\tAccuracy: 8869/10000 (88%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.2558\tAccuracy: 45324/50000 (90%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLosses xa_bn_do: 0.212985\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tLosses xa_bn_do: 0.282459\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tLosses xa_bn_do: 0.266677\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tLosses xa_bn_do: 0.199570\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tLosses xa_bn_do: 0.223269\n",
            "Train Epoch: 6 [50000/50000 (100%)]\tLosses xa_bn_do: 0.233373\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3181\tAccuracy: 8864/10000 (88%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.2362\tAccuracy: 45757/50000 (91%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLosses xa_bn_do: 0.163390\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tLosses xa_bn_do: 0.246228\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tLosses xa_bn_do: 0.263193\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tLosses xa_bn_do: 0.163132\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tLosses xa_bn_do: 0.238066\n",
            "Train Epoch: 7 [50000/50000 (100%)]\tLosses xa_bn_do: 0.338486\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3391\tAccuracy: 8799/10000 (87%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.2417\tAccuracy: 45516/50000 (91%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLosses xa_bn_do: 0.171860\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tLosses xa_bn_do: 0.242308\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tLosses xa_bn_do: 0.220964\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tLosses xa_bn_do: 0.475299\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tLosses xa_bn_do: 0.267343\n",
            "Train Epoch: 8 [50000/50000 (100%)]\tLosses xa_bn_do: 0.101823\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3117\tAccuracy: 8918/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.2104\tAccuracy: 46197/50000 (92%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLosses xa_bn_do: 0.147570\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tLosses xa_bn_do: 0.430998\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tLosses xa_bn_do: 0.153932\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tLosses xa_bn_do: 0.213554\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tLosses xa_bn_do: 0.262669\n",
            "Train Epoch: 9 [50000/50000 (100%)]\tLosses xa_bn_do: 0.179278\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3115\tAccuracy: 8878/10000 (88%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1996\tAccuracy: 46372/50000 (92%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLosses xa_bn_do: 0.107880\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tLosses xa_bn_do: 0.196036\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tLosses xa_bn_do: 0.303829\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tLosses xa_bn_do: 0.159128\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tLosses xa_bn_do: 0.210117\n",
            "Train Epoch: 10 [50000/50000 (100%)]\tLosses xa_bn_do: 0.191125\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3048\tAccuracy: 8964/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1807\tAccuracy: 46785/50000 (93%)\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLosses xa_bn_do: 0.136503\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tLosses xa_bn_do: 0.281339\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tLosses xa_bn_do: 0.165449\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tLosses xa_bn_do: 0.386151\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tLosses xa_bn_do: 0.251888\n",
            "Train Epoch: 11 [50000/50000 (100%)]\tLosses xa_bn_do: 0.358323\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3105\tAccuracy: 8937/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1750\tAccuracy: 46798/50000 (93%)\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLosses xa_bn_do: 0.147402\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tLosses xa_bn_do: 0.088113\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tLosses xa_bn_do: 0.254467\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tLosses xa_bn_do: 0.167709\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tLosses xa_bn_do: 0.152875\n",
            "Train Epoch: 12 [50000/50000 (100%)]\tLosses xa_bn_do: 0.090587\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3004\tAccuracy: 8970/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1594\tAccuracy: 47212/50000 (94%)\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLosses xa_bn_do: 0.161991\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tLosses xa_bn_do: 0.130409\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tLosses xa_bn_do: 0.179241\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tLosses xa_bn_do: 0.197100\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tLosses xa_bn_do: 0.141672\n",
            "Train Epoch: 13 [50000/50000 (100%)]\tLosses xa_bn_do: 0.215281\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3063\tAccuracy: 8947/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1538\tAccuracy: 47326/50000 (94%)\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLosses xa_bn_do: 0.224631\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tLosses xa_bn_do: 0.183164\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tLosses xa_bn_do: 0.293604\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tLosses xa_bn_do: 0.116511\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tLosses xa_bn_do: 0.185162\n",
            "Train Epoch: 14 [50000/50000 (100%)]\tLosses xa_bn_do: 0.154763\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3118\tAccuracy: 8941/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1505\tAccuracy: 47330/50000 (94%)\n",
            "\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLosses xa_bn_do: 0.190839\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tLosses xa_bn_do: 0.129652\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tLosses xa_bn_do: 0.312951\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tLosses xa_bn_do: 0.066372\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tLosses xa_bn_do: 0.141430\n",
            "Train Epoch: 15 [50000/50000 (100%)]\tLosses xa_bn_do: 0.229326\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3087\tAccuracy: 8937/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1364\tAccuracy: 47595/50000 (95%)\n",
            "\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLosses xa_bn_do: 0.229873\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tLosses xa_bn_do: 0.049126\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tLosses xa_bn_do: 0.105357\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tLosses xa_bn_do: 0.049483\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tLosses xa_bn_do: 0.185212\n",
            "Train Epoch: 16 [50000/50000 (100%)]\tLosses xa_bn_do: 0.151434\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3059\tAccuracy: 8966/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1315\tAccuracy: 47743/50000 (95%)\n",
            "\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLosses xa_bn_do: 0.134560\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tLosses xa_bn_do: 0.308585\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tLosses xa_bn_do: 0.054662\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tLosses xa_bn_do: 0.126662\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tLosses xa_bn_do: 0.225387\n",
            "Train Epoch: 17 [50000/50000 (100%)]\tLosses xa_bn_do: 0.124989\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3064\tAccuracy: 8956/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1203\tAccuracy: 48007/50000 (96%)\n",
            "\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLosses xa_bn_do: 0.153599\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tLosses xa_bn_do: 0.161340\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tLosses xa_bn_do: 0.178080\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tLosses xa_bn_do: 0.234132\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tLosses xa_bn_do: 0.112529\n",
            "Train Epoch: 18 [50000/50000 (100%)]\tLosses xa_bn_do: 0.105976\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3239\tAccuracy: 8936/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1218\tAccuracy: 47838/50000 (95%)\n",
            "\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLosses xa_bn_do: 0.132704\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tLosses xa_bn_do: 0.129864\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tLosses xa_bn_do: 0.094236\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tLosses xa_bn_do: 0.165103\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tLosses xa_bn_do: 0.160721\n",
            "Train Epoch: 19 [50000/50000 (100%)]\tLosses xa_bn_do: 0.153594\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3242\tAccuracy: 8962/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1152\tAccuracy: 47923/50000 (95%)\n",
            "\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLosses xa_bn_do: 0.117720\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tLosses xa_bn_do: 0.178085\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tLosses xa_bn_do: 0.116356\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tLosses xa_bn_do: 0.141846\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tLosses xa_bn_do: 0.099245\n",
            "Train Epoch: 20 [50000/50000 (100%)]\tLosses xa_bn_do: 0.119248\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3179\tAccuracy: 8984/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1066\tAccuracy: 48216/50000 (96%)\n",
            "\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLosses xa_bn_do: 0.109460\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tLosses xa_bn_do: 0.170704\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tLosses xa_bn_do: 0.166806\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tLosses xa_bn_do: 0.090084\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tLosses xa_bn_do: 0.242050\n",
            "Train Epoch: 21 [50000/50000 (100%)]\tLosses xa_bn_do: 0.188177\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3250\tAccuracy: 8980/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1046\tAccuracy: 48178/50000 (96%)\n",
            "\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLosses xa_bn_do: 0.088446\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tLosses xa_bn_do: 0.115543\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tLosses xa_bn_do: 0.092052\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tLosses xa_bn_do: 0.199592\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tLosses xa_bn_do: 0.073050\n",
            "Train Epoch: 22 [50000/50000 (100%)]\tLosses xa_bn_do: 0.152635\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3336\tAccuracy: 8941/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.1032\tAccuracy: 48176/50000 (96%)\n",
            "\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLosses xa_bn_do: 0.098621\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tLosses xa_bn_do: 0.115025\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tLosses xa_bn_do: 0.263709\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tLosses xa_bn_do: 0.116940\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tLosses xa_bn_do: 0.078744\n",
            "Train Epoch: 23 [50000/50000 (100%)]\tLosses xa_bn_do: 0.052196\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3297\tAccuracy: 8972/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0916\tAccuracy: 48471/50000 (96%)\n",
            "\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLosses xa_bn_do: 0.052013\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tLosses xa_bn_do: 0.149083\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tLosses xa_bn_do: 0.053729\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tLosses xa_bn_do: 0.068727\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tLosses xa_bn_do: 0.156355\n",
            "Train Epoch: 24 [50000/50000 (100%)]\tLosses xa_bn_do: 0.167902\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3305\tAccuracy: 9007/10000 (90%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0850\tAccuracy: 48623/50000 (97%)\n",
            "\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLosses xa_bn_do: 0.157133\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tLosses xa_bn_do: 0.117723\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tLosses xa_bn_do: 0.125685\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tLosses xa_bn_do: 0.117937\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tLosses xa_bn_do: 0.200527\n",
            "Train Epoch: 25 [50000/50000 (100%)]\tLosses xa_bn_do: 0.118904\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3589\tAccuracy: 8940/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0980\tAccuracy: 48257/50000 (96%)\n",
            "\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLosses xa_bn_do: 0.057713\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tLosses xa_bn_do: 0.125811\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tLosses xa_bn_do: 0.080327\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tLosses xa_bn_do: 0.096055\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tLosses xa_bn_do: 0.070454\n",
            "Train Epoch: 26 [50000/50000 (100%)]\tLosses xa_bn_do: 0.085212\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3386\tAccuracy: 8977/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0778\tAccuracy: 48736/50000 (97%)\n",
            "\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLosses xa_bn_do: 0.058288\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tLosses xa_bn_do: 0.069320\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tLosses xa_bn_do: 0.092848\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tLosses xa_bn_do: 0.060711\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tLosses xa_bn_do: 0.020972\n",
            "Train Epoch: 27 [50000/50000 (100%)]\tLosses xa_bn_do: 0.134724\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3448\tAccuracy: 8999/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0737\tAccuracy: 48788/50000 (97%)\n",
            "\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLosses xa_bn_do: 0.101426\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tLosses xa_bn_do: 0.025023\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tLosses xa_bn_do: 0.056253\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tLosses xa_bn_do: 0.142719\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tLosses xa_bn_do: 0.044711\n",
            "Train Epoch: 28 [50000/50000 (100%)]\tLosses xa_bn_do: 0.050001\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3482\tAccuracy: 8961/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0683\tAccuracy: 48940/50000 (97%)\n",
            "\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLosses xa_bn_do: 0.128552\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tLosses xa_bn_do: 0.058585\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tLosses xa_bn_do: 0.051157\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tLosses xa_bn_do: 0.084027\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tLosses xa_bn_do: 0.102875\n",
            "Train Epoch: 29 [50000/50000 (100%)]\tLosses xa_bn_do: 0.100034\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3537\tAccuracy: 8963/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0704\tAccuracy: 48890/50000 (97%)\n",
            "\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLosses xa_bn_do: 0.126797\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tLosses xa_bn_do: 0.036066\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tLosses xa_bn_do: 0.200459\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tLosses xa_bn_do: 0.087125\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tLosses xa_bn_do: 0.082187\n",
            "Train Epoch: 30 [50000/50000 (100%)]\tLosses xa_bn_do: 0.015208\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3533\tAccuracy: 8997/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0693\tAccuracy: 48872/50000 (97%)\n",
            "\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLosses xa_bn_do: 0.087159\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tLosses xa_bn_do: 0.069444\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tLosses xa_bn_do: 0.050072\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tLosses xa_bn_do: 0.108911\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tLosses xa_bn_do: 0.150875\n",
            "Train Epoch: 31 [50000/50000 (100%)]\tLosses xa_bn_do: 0.094531\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3555\tAccuracy: 8981/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0647\tAccuracy: 48982/50000 (97%)\n",
            "\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLosses xa_bn_do: 0.055748\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tLosses xa_bn_do: 0.108098\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tLosses xa_bn_do: 0.038560\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tLosses xa_bn_do: 0.076249\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tLosses xa_bn_do: 0.120057\n",
            "Train Epoch: 32 [50000/50000 (100%)]\tLosses xa_bn_do: 0.121012\n",
            "Test set:\n",
            "xa_bn_do: Loss: 0.3678\tAccuracy: 8977/10000 (89%)\n",
            "\n",
            "Train set:\n",
            "xa_bn_do: Loss: 0.0620\tAccuracy: 48958/50000 (97%)\n",
            "\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLosses xa_bn_do: 0.041635\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tLosses xa_bn_do: 0.104468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-a6a24cc612fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-14657e799368>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, models)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JqSsjWMrJNaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "union_log = {}\n",
        "for k in test_log.keys():\n",
        "  union_log[k+\"_test\"]=test_log[k]\n",
        "for k in train_log.keys():\n",
        "  union_log[k+\"_train\"]=train_log[k]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aHLX6qnkJgNV",
        "colab_type": "code",
        "outputId": "ae9d48f1-6e00-4e47-aceb-094d49b11f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(union_log, 'loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2d263ac048>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}