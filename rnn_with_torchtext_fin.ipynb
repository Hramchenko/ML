{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn_with_torchtext_fin.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"onHB9k9IOrJK","colab_type":"code","colab":{}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision torchtext spacy\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4Jj8CARBR_z4","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"AE2FWX7ISAj-","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"rV62S3z0OgNd","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","\n","import itertools\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import math \n","\n","import sys\n","sys.path.append('../')\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o3ns0sxdOgNg","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch.autograd import Variable as V\n","import torchtext\n","from torchtext import data\n","import spacy\n","import numpy as np\n","from tqdm import tqdm \n"," \n","\n","from torchtext.datasets import WikiText2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lJ5Me6E1OgNi","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","TEXT = data.Field(lower=True, tokenize=lambda x: list(x))\n","\n","device = 'cuda'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ShEYSLaGOgNk","colab_type":"code","colab":{}},"cell_type":"code","source":["train, valid, test = WikiText2.splits(TEXT)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1bRdys7TOgNn","colab_type":"code","colab":{}},"cell_type":"code","source":["#TEXT.build_vocab(train, vectors=\"glove.6B.100d\")\n","TEXT.build_vocab(train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QwNsoqj-OgNs","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 128\n","sequence_length = 30\n","grad_clip = 0.1\n","lr = 4.\n","best_val_loss = None\n","log_interval = 100\n","eval_batch_size = 128"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WLkk0biAOgNu","colab_type":"code","colab":{}},"cell_type":"code","source":["train_loader, val_loader, test_loader = data.BPTTIterator.splits(\n","    (train, valid, test),\n","    batch_size=batch_size,\n","    bptt_len=sequence_length,\n","    device=device,\n","    repeat=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IXtgGsCyOgNx","colab_type":"code","colab":{}},"cell_type":"code","source":["batch, (torchtext_data) = next(enumerate(train_loader))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I5NCWVY5OgNz","colab_type":"code","colab":{}},"cell_type":"code","source":["data = torchtext_data.text"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s3bSm2qaOgN2","colab_type":"code","colab":{}},"cell_type":"code","source":["targets = torchtext_data.target"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yLPQ2gS7OgN4","colab_type":"code","outputId":"8d8cc546-087a-412b-9383-d543ef106a6d","executionInfo":{"status":"ok","timestamp":1546931424239,"user_tz":-300,"elapsed":352,"user":{"displayName":"Виталий Храмченко","photoUrl":"","userId":"10935595370719524564"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["data[:5, :3]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2,  9, 16],\n","        [30, 22, 22],\n","        [ 2,  2,  4],\n","        [33, 20, 11],\n","        [ 2,  5, 10]], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"7L2ZAJCeOgN6","colab_type":"code","outputId":"8b747e0e-a880-4494-d702-f5fc6b568b20","executionInfo":{"status":"ok","timestamp":1546931425605,"user_tz":-300,"elapsed":543,"user":{"displayName":"Виталий Храмченко","photoUrl":"","userId":"10935595370719524564"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["targets.reshape(30, 128)[:5, :3]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[30, 22, 22],\n","        [ 2,  2,  4],\n","        [33, 20, 11],\n","        [ 2,  5, 10],\n","        [26, 10,  2]], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"Sfz9W2S-OgN9","colab_type":"code","colab":{}},"cell_type":"code","source":["class RNNModel(nn.Module):\n","\n","    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n","        super(RNNModel, self).__init__()\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        if rnn_type == 'LSTM':\n","            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n","        elif rnn_type == 'GRU':\n","            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n","        self.decoder = nn.Linear(nhid, ntoken)\n","\n","        self.init_weights()\n","\n","        self.rnn_type = rnn_type\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, x, hidden=None):\n","        emb = self.drop(self.encoder(x))\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n","        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters()).data\n","        if self.rnn_type == 'LSTM':\n","            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n","                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n","        else:\n","            return weight.new(self.nlayers, bsz, self.nhid).zero_()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U-o3-GvFOgN_","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate(data_loader):\n","    model.eval()\n","    total_loss = 0\n","    ntokens = len(TEXT.vocab)\n","    hidden = model.init_hidden(eval_batch_size)\n","    for i, (torchtext_data) in enumerate(data_loader):\n","        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n","        data = data.to(device)\n","        targets = targets.to(device)\n","        output, hidden = model(data)\n","        output_flat = output.view(-1, ntokens)\n","        total_loss += len(data) * criterion(output_flat, targets).item()\n","    return total_loss / len(data_loader)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d130094kOgOB","colab_type":"code","colab":{}},"cell_type":"code","source":["def train():\n","    model.train()\n","    total_loss = 0\n","#     for batch in tqdm(train_loader):\n","#     # reset the hidden state or else the model will try to backpropagate to the\n","#     # beginning of the dataset, requiring lots of time and a lot of memory\n","    #model.reset_history()\n","    ntokens = len(TEXT.vocab)\n","    for batch, (torchtext_data) in enumerate(train_loader):\n","        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n","        data = data.to(device)\n","        targets = targets.to(device)\n","        model.zero_grad()\n","        output, hidden = model(data)\n","        loss = criterion(output.view(-1, ntokens), targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","        for p in model.parameters():\n","            p.data.add_(-lr, p.grad.data)\n","\n","        total_loss += loss.item()\n","\n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"11M2me0aOgOD","colab_type":"code","outputId":"8cf867e0-9231-4121-a5d0-f1188f7879c5","executionInfo":{"status":"ok","timestamp":1546931429428,"user_tz":-300,"elapsed":359,"user":{"displayName":"Виталий Храмченко","photoUrl":"","userId":"10935595370719524564"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["len(TEXT.vocab)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["245"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"nS0d_sXZOgOI","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","ntokens = len(TEXT.vocab)\n","ninp = 128\n","#model = RNNModel('LSTM', ntokens, ninp, 128, 2, batch_size, 0.3)\n","model = RNNModel('LSTM', ntokens, 128, 128, 3, 0.3).to(device)\n","\n","#model.encoder.weight.data.copy_(weight_matrix)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Std2PQp8OgOK","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate(n=50, temp=1.):\n","    model.eval()\n","    x = torch.rand(1, 1).mul(ntokens).long().cuda()\n","    hidden = None\n","    out = []\n","    for i in range(n):\n","        output, hidden = model(x, hidden)\n","        s_weights = output.squeeze().data.div(temp).exp()\n","        s_idx = torch.multinomial(s_weights, 1)[0]\n","        x.data.fill_(s_idx)\n","        s = TEXT.vocab.itos[s_idx]\n","        out.append(s)\n","    return ''.join(out)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5gLrpRPNOgON","colab_type":"code","outputId":"f4d4e5ee-7d96-4236-fda8-e10249cb3de7","executionInfo":{"status":"error","timestamp":1546932061692,"user_tz":-300,"elapsed":606980,"user":{"displayName":"Виталий Храмченко","photoUrl":"","userId":"10935595370719524564"}},"colab":{"base_uri":"https://localhost:8080/","height":4740}},"cell_type":"code","source":["with torch.no_grad():\n","    print('sample:\\n', generate(50), '\\n')\n","\n","for epoch in range(1, 30):\n","    train()\n","    val_loss = evaluate(val_loader)\n","    print('-' * 89)\n","    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n","        epoch, val_loss, math.exp(val_loss)))\n","    print('-' * 89)\n","    if not best_val_loss or val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","    else:\n","        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","        lr /= 4.0\n","    with torch.no_grad():\n","        print('sample:\\n', generate(50), '\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["sample:\n"," eصc=dem iiié7r=cef eieṯtemiuincu uuī etom,tcžloxai \n","\n","| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.13 | ppl    22.89\n","| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.96\n","| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.93\n","| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  3.08 | ppl    21.74\n","| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.92\n","| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  3.08 | ppl    21.85\n","| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.88\n","| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  3.08 | ppl    21.74\n","| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.90\n","| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  3.08 | ppl    21.84\n","| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  3.01 | ppl    20.37\n","| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.90 | ppl    18.25\n","| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.83 | ppl    16.97\n","| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.74 | ppl    15.43\n","| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.67 | ppl    14.40\n","| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.61 | ppl    13.59\n","| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.56 | ppl    12.89\n","| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.52 | ppl    12.41\n","| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.48 | ppl    12.00\n","| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.45 | ppl    11.62\n","| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.42 | ppl    11.29\n","| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.39 | ppl    10.93\n","| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.36 | ppl    10.64\n","| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.33 | ppl    10.28\n","| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.31 | ppl    10.10\n","| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.28 | ppl     9.82\n","| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.27 | ppl     9.64\n","| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.24 | ppl     9.36\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | valid loss 62.44 | valid ppl 1306966219438120896754089984.00\n","-----------------------------------------------------------------------------------------\n","sample:\n"," l ' = aldaren . at a mezimtfeclavmantiin roisenter \n","\n","| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.25 | ppl     9.44\n","| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.20 | ppl     9.00\n","| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.18 | ppl     8.88\n","| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  2.17 | ppl     8.76\n","| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  2.15 | ppl     8.59\n","| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.46\n","| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  2.12 | ppl     8.37\n","| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  2.11 | ppl     8.27\n","| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.20\n","| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  2.09 | ppl     8.07\n","| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.92\n","| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.86\n","| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  2.05 | ppl     7.78\n","| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  2.03 | ppl     7.64\n","| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  2.03 | ppl     7.61\n","| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.56\n","| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.47\n","| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.42\n","| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.40\n","| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.27\n","| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.26\n","| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.19\n","| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.17\n","| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.06\n","| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.04\n","| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.01\n","| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.94\n","| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | valid loss 52.02 | valid ppl 39167503105377339179008.00\n","-----------------------------------------------------------------------------------------\n","sample:\n"," e <unk> , a bavition state <unk> 5 xmolica over br \n","\n","| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.96\n","| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.73\n","| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.71\n","| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.70\n","| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.64\n","| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.59\n","| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.59\n","| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.55\n","| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.53\n","| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.52\n","| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.44\n","| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.45\n","| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.41\n","| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.29\n","| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.33\n","| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.32\n","| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.29\n","| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.27\n","| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.30\n","| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.21\n","| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.23\n","| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.19\n","| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.21\n","| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.12\n","| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.12\n","| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.15\n","| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.09\n","| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | valid loss 47.65 | valid ppl 492367271662587936768.00\n","-----------------------------------------------------------------------------------------\n","sample:\n","  collevires lowral michloge . <eos> nune not 1927 coly \n","\n","| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.13\n","| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.99\n","| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.98\n","| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.99\n","| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.95\n","| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.93\n","| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n","| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.92\n","| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.93\n","| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.91\n","| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n","| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n","| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.84\n","| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n","| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n","| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n","| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n","| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n","| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n","| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n","| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n","| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n","| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n","| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n","| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n","| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n","| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n","| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | valid loss 45.59 | valid ppl 63127535490182332416.00\n","-----------------------------------------------------------------------------------------\n","sample:\n"," orth character , <eos> = = = = <eos> suppecber the was wit \n","\n","| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n","| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n","| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n","| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n","| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.61\n","| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n","| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n","| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n","| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.61\n","| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n","| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n","| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n","| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n","| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n","| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n","| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n","| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n","| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n","| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n","| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n","| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n","| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n","| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n","| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n","| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n","| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n","| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n","| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | valid loss 44.44 | valid ppl 20022594330308521984.00\n","-----------------------------------------------------------------------------------------\n","sample:\n"," > ) and with which feltur in her ordania . domandr \n","\n","| epoch   6 |   100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n","| epoch   6 |   200/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n","| epoch   6 |   300/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n","| epoch   6 |   400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n","| epoch   6 |   500/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n","| epoch   6 |   600/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n","| epoch   6 |   700/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n","| epoch   6 |   800/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n","| epoch   6 |   900/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n","| epoch   6 |  1000/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n","| epoch   6 |  1100/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n","| epoch   6 |  1200/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n","| epoch   6 |  1300/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n","| epoch   6 |  1400/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n","| epoch   6 |  1500/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n","| epoch   6 |  1600/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.37\n","| epoch   6 |  1700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.34\n","| epoch   6 |  1800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n","| epoch   6 |  1900/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n","| epoch   6 |  2000/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.33\n","| epoch   6 |  2100/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n","| epoch   6 |  2200/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n","| epoch   6 |  2300/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n","| epoch   6 |  2400/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n","| epoch   6 |  2500/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.33\n","| epoch   6 |  2600/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.37\n","| epoch   6 |  2700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.34\n","| epoch   6 |  2800/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | valid loss 43.67 | valid ppl 9276263863184738304.00\n","-----------------------------------------------------------------------------------------\n","sample:\n"," egones , which was restricted that the contrort th \n","\n","| epoch   7 |   100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n","| epoch   7 |   200/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n","| epoch   7 |   300/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n","| epoch   7 |   400/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n","| epoch   7 |   500/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n","| epoch   7 |   600/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n","| epoch   7 |   700/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n","| epoch   7 |   800/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.28\n","| epoch   7 |   900/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n","| epoch   7 |  1000/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n","| epoch   7 |  1100/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n","| epoch   7 |  1200/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.29\n","| epoch   7 |  1300/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n","| epoch   7 |  1400/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.18\n","| epoch   7 |  1500/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.23\n","| epoch   7 |  1600/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n","| epoch   7 |  1700/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.23\n","| epoch   7 |  1800/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.24\n","| epoch   7 |  1900/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n","| epoch   7 |  2000/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.23\n","| epoch   7 |  2100/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n","| epoch   7 |  2200/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n","| epoch   7 |  2300/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n","| epoch   7 |  2400/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.20\n","| epoch   7 |  2500/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.21\n","| epoch   7 |  2600/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n","| epoch   7 |  2700/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.24\n","| epoch   7 |  2800/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.18\n","-----------------------------------------------------------------------------------------\n","| end of epoch   7 | valid loss 43.08 | valid ppl 5128362715141251072.00\n","-----------------------------------------------------------------------------------------\n","sample:\n"," ebory death family perved the eegise who becoming  \n","\n","| epoch   8 |   100/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n","| epoch   8 |   200/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.18\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-59df1ca0ab2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-0c58bd642c4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"6VAtEYaXOgOQ","colab_type":"code","outputId":"c37d2549-a832-4759-ebbd-503f6d4b6918","colab":{}},"cell_type":"code","source":["print('sample:\\n', generate_(50), '\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["sample:\n","  thit dab frane the rixings apranto , tros she ter \n","\n"],"name":"stdout"}]},{"metadata":{"id":"bQVEzKmuOgOS","colab_type":"code","outputId":"9d4718a1-56eb-4ab3-d4ac-433befd5ef9a","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'s'"]},"metadata":{"tags":[]},"execution_count":103}]},{"metadata":{"id":"O7pezHP-OgOU","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}