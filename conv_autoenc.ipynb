{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_autoenc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/ML/blob/master/conv_autoenc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AcikXdE65XEa",
        "colab_type": "code",
        "outputId": "45997992-dff3-4f4d-a097-befc7cbe2969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip3 install torch torchvision\n",
        "import torch\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EFGxZrIN55Gu",
        "colab_type": "code",
        "outputId": "a6d6d757-d6b8-4619-8383-4eafab92d686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qlnZKtsI60MF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mnist_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "           ])\n",
        "\n",
        "def mnist(batch_size=50, valid=0, shuffle=True, transform=mnist_transform, path='./MNIST_data'):\n",
        "    test_data = datasets.MNIST(path, train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    train_data = datasets.MNIST(path, train=True, download=True, transform=transform)\n",
        "    if valid > 0:\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = num_train-valid\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "        return train_loader, valid_loader, test_loader\n",
        "    else:\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "\n",
        "def plot_mnist(images, shape, show=True, save_to=None):\n",
        "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
        "    for j in range(1, len(images) + 1):\n",
        "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
        "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "\n",
        "    if save_to is not None:\n",
        "        plt.savefig(save_to)\n",
        "        \n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    \n",
        "def plot_graphs(log, tpe='loss'):\n",
        "    keys = log.keys()\n",
        "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
        "    epochs = {k:range(len(log[k])) for k in keys}\n",
        "    \n",
        "    if tpe == 'loss':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
        "        plt.title('errors')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('error')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()\n",
        "    elif tpe == 'accuracy':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
        "        plt.title('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XYQ3LtbR4_R1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcJB_TXD4_R5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiLhMCYg4_R7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_folder = 'Conv_VAE_log_results'\n",
        "fixed_folder = root_folder + '/Fixed_results'\n",
        "AE_folder = root_folder + '/AE_results'\n",
        "\n",
        "if os.path.isdir(root_folder):\n",
        "    !rm -r $root_folder\n",
        "os.mkdir(root_folder)\n",
        "os.mkdir(fixed_folder)\n",
        "os.mkdir(AE_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8NQSfD7M4_R-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_tanh = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)),\n",
        "           ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gE-145Pf4_R_",
        "colab_type": "code",
        "outputId": "6dfb38c9-38aa-4578-8a2f-12011b136f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = mnist(valid=10000, transform=mnist_tanh)\n",
        "fixed_z = torch.randn((50, 2))\n",
        "fixed_data, _ = next(iter(test_loader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cuhqcdzb4_SB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHA_Dopk4_SE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0bB3qL814_SG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWaki7N-4_SI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, latent_size=32, gamma=1e-3, batchnorm=False, dropout=False, optim_type='SGD', **optim_params):\n",
        "        super(Net, self).__init__()\n",
        "    \n",
        "        self.latent_size = latent_size\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        self._conv1 = ConvLayer([1, 16, 4], padding=0, bn=batchnorm, stride=2, pool_layer=None)\n",
        "        self._conv2 = ConvLayer([16, 32, 4], padding=0, bn=batchnorm, stride=2, pool_layer=None)\n",
        "        self._conv3 = ConvLayer([32, 32, 3], padding=0, bn=batchnorm, stride=2, pool_layer=None)\n",
        "            \n",
        "        self.fce = FullyConnected([32*2*2, latent_size])\n",
        "        self.fcd = FullyConnected([latent_size, 32*2*2])\n",
        "        \n",
        "        self._deconv1 = DeconvLayer([32, 32, 3], stride=2, padding=0, bn=batchnorm)\n",
        "        self._deconv2 = DeconvLayer([32, 16, 4], stride=2, padding=0, bn=batchnorm, output_padding=1)\n",
        "        self._deconv3 = DeconvLayer([16, 1, 4], stride=2, padding=0, bn=batchnorm, activation_fn=nn.Tanh())\n",
        "        \n",
        "        self._loss = None\n",
        "        if optim_type == 'SGD':\n",
        "            self.optim = optim.SGD(self.parameters(), **optim_params)\n",
        "        elif optim_type == 'Adadelta':\n",
        "            self.optim = optim.Adadelta(self.parameters(), **optim_params)\n",
        "        elif optim_type == 'RMSProp':\n",
        "            self.optim = optim.RMSprop(self.parameters(), **optim_params)\n",
        "        elif optim_type == 'Adam':\n",
        "            self.optim = optim.Adam(self.parameters(), **optim_params)\n",
        "        \n",
        "    def conv(self, x):\n",
        "        l1 = self._conv1(x)\n",
        "        l2 = self._conv2(l1)\n",
        "        l3 = self._conv3(l2)\n",
        "        return l3, l2, l1\n",
        "    \n",
        "    def encode(self, x):\n",
        "        l3 = self.conv(x)[0]\n",
        "        flatten = l3.view(-1, 32*2*2)\n",
        "        h = self.fce(flatten)\n",
        "        return h\n",
        "    \n",
        "    def decode(self, h):\n",
        "        flatten = self.fcd(h)\n",
        "        l1 = flatten.view(-1, 32, 2, 2)\n",
        "        l2 = self._deconv1(l1)\n",
        "        l3 = self._deconv2(l2)\n",
        "        x = self._deconv3(l3)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.encode(x)\n",
        "        r = self.decode(h)\n",
        "        return r\n",
        "    \n",
        "    def _sample(self, n):\n",
        "        return torch.randn([n, self.latent_size]).to(device)\n",
        "    \n",
        "    def loss(self, output, target, size_average=True):\n",
        "        self._loss = F.mse_loss(output, target)\n",
        "        return self._loss\n",
        "        \n",
        "        \n",
        "        KLD = self.mu**2 + self.logvar.exp() - 1 - self.logvar\n",
        "        KLD = 0.5*KLD.sum(1)\n",
        "        if size_average:\n",
        "            self.KLD = KLD.mean()\n",
        "            self._loss = F.mse_loss(output, target, reduction='mean')\n",
        "        else:\n",
        "            self.KLD = KLD.sum()\n",
        "            self._loss = F.mse_loss(output, target, reduction='sum')\n",
        "        \n",
        "        return self._loss + self.gamma*self.KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9lth0hX4_SL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "models = {'VAE10': Net(10, 0.005, batchnorm=True, dropout=False, optim_type='Adam', lr=1e-4).to(device)}\n",
        "train_log = {k: [] for k in models}\n",
        "test_log = {k: [] for k in models}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0Td_RJW4_SM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, models, log=None):\n",
        "    train_size = len(train_loader.sampler)\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        print(data.shape)\n",
        "        for model in models.values():\n",
        "            model.optim.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, data)\n",
        "            loss.backward()\n",
        "            model.optim.step()\n",
        "            \n",
        "        if batch_idx % 200 == 0:\n",
        "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
        "            losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "            print(line + losses)\n",
        "            \n",
        "    else:\n",
        "        batch_idx += 1\n",
        "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
        "        losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "        if log is not None:\n",
        "            for k in models:\n",
        "                log[k].append((models[k]._loss,))\n",
        "        print(line + losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-yyxNfl4_SO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(models, loader, epoch, log=None):\n",
        "    test_size = len(loader)\n",
        "    test_reconstruction_loss = {k: 0. for k in models}\n",
        "    test_KL_loss = {k: 0. for k in models}\n",
        "    with torch.no_grad():\n",
        "        for data, _ in loader:\n",
        "           \n",
        "            data = data.to(device)\n",
        "            output = {k: m(data) for k, m in models.items()}\n",
        "            for k, m in models.items():\n",
        "                m.loss(output[k], data)\n",
        "                test_reconstruction_loss[k] += m._loss.item()\n",
        "#                test_KL_loss[k] += m.KLD.item()\n",
        "    \n",
        "    for k in models:\n",
        "        test_reconstruction_loss[k] /= test_size\n",
        "        test_KL_loss[k] /= test_size\n",
        "    report = 'Test losses: ' + ' '.join(['{}: {:.4f}/{:.4f}'.format(k, \n",
        "                                                                    test_reconstruction_loss[k], \n",
        "                                                                    test_KL_loss[k]) for k in models])\n",
        "    for k in models:\n",
        "        if log is not None:\n",
        "            log[k].append((test_reconstruction_loss[k], test_KL_loss[k]))\n",
        "        with torch.no_grad():\n",
        "            ae_data = models[k](fixed_data.to(device)).data.cpu().numpy()\n",
        "            #fixed_gen = models[k].decode(fixed_z.to(device)).data.cpu().numpy()\n",
        "            plot_mnist(fixed_data, (5, 10), True, AE_folder + '/%s_%03d.png' % (k, epoch))\n",
        "            plot_mnist(ae_data, (5, 10), True, AE_folder + '/%s_%03d.png' % (k, epoch))\n",
        "            #plot_mnist(fixed_gen, (5, 10), True, fixed_folder + '/%s_%03d.png' % (k, epoch))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DAcwiTY4_SR",
        "colab_type": "code",
        "outputId": "3196d8cb-8a60-4f8a-a5ae-1c991ecac3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1450
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 31):\n",
        "    for model in models.values():\n",
        "        model.train()\n",
        "    train(epoch, models, train_log)\n",
        "    for model in models.values():\n",
        "        model.eval()\n",
        "    test(models, valid_loader, epoch, test_log)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 1, 28, 28])\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLosses VAE10: 0.7331\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n",
            "torch.Size([50, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cb239923c37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-b5d599a4ea49>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, models, log)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uUORqNjo4_SV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.linspace(-3, 3, 10)\n",
        "y = np.linspace(-3, 3, 10)\n",
        "xv, yv = np.meshgrid(x, y)\n",
        "NPA = np.hstack([xv.reshape([1,-1]).T, yv.reshape([1,-1]).T])\n",
        "r = torch.FloatTensor(NPA)\n",
        "with torch.no_grad():\n",
        "  fixed_gen = models['VAE10'].decode(r.to(device)).data.cpu().numpy()\n",
        "  plot_mnist(fixed_gen, (10, 10), True, fixed_folder + '/%s_%03d.png' % (0, epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AkRzTx8J4_SX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}